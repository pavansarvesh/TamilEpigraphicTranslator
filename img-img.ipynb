{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5888ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17125 files belonging to 25 classes.\n",
      "Found 3675 files belonging to 25 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 44, 44, 128), (None, 45, 45, 128)]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m models.Model(inputs, outputs)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# --- COMPILE ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m model = \u001b[43mbuild_image_to_image_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# --- TRAIN ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mbuild_image_to_image_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     65\u001b[39m x = layers.Conv2DTranspose(\u001b[32m128\u001b[39m, \u001b[32m3\u001b[39m, strides=\u001b[32m2\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m, padding=\u001b[33m'\u001b[39m\u001b[33msame\u001b[39m\u001b[33m'\u001b[39m)(x)\n\u001b[32m     66\u001b[39m x = layers.Cropping2D(((\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m), (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m)))(x)  \u001b[38;5;66;03m# Adjust shape to match skip3\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m x = \u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m x = layers.Conv2D(\u001b[32m128\u001b[39m, \u001b[32m3\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m, padding=\u001b[33m'\u001b[39m\u001b[33msame\u001b[39m\u001b[33m'\u001b[39m)(x)\n\u001b[32m     70\u001b[39m x = layers.Conv2DTranspose(\u001b[32m64\u001b[39m, \u001b[32m3\u001b[39m, strides=\u001b[32m2\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m, padding=\u001b[33m'\u001b[39m\u001b[33msame\u001b[39m\u001b[33m'\u001b[39m)(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Pavan Sarvesh/tf-env/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Pavan Sarvesh/tf-env/lib/python3.12/site-packages/keras/src/layers/merging/concatenate.py:99\u001b[39m, in \u001b[36mConcatenate.build\u001b[39m\u001b[34m(self, input_shape)\u001b[39m\n\u001b[32m     93\u001b[39m         unique_dims = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m     94\u001b[39m             shape[axis]\n\u001b[32m     95\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set\n\u001b[32m     96\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     97\u001b[39m         )\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) > \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.built = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 44, 44, 128), (None, 45, 45, 128)]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input #type:ignore\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# CONFIG\n",
    "img_height, img_width = 180, 180\n",
    "batch_size = 32\n",
    "num_classes = 26  # 00 to 25\n",
    "\n",
    "# --- LOAD TEMPLATE IMAGES ---\n",
    "template_images = {}\n",
    "for class_idx in range(num_classes):\n",
    "    path = f\"./templates/{class_idx:02d}.png\"\n",
    "    img = tf.keras.utils.load_img(path, target_size=(img_height, img_width))\n",
    "    img = tf.keras.utils.img_to_array(img) / 255.0\n",
    "    template_images[class_idx] = img\n",
    "\n",
    "# --- DATASET LOADING ---\n",
    "def load_dataset(path):\n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        shuffle=True,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        label_mode='int'\n",
    "    )\n",
    "\n",
    "data_train = load_dataset('./dataset/newDatasetSplit/train')\n",
    "data_val = load_dataset('./dataset/newDatasetSplit/val')\n",
    "\n",
    "# --- MAP EACH INPUT TO ITS TEMPLATE IMAGE ---\n",
    "def map_to_target(img, label):\n",
    "    target_img = tf.convert_to_tensor([template_images[int(label[i].numpy())] for i in range(len(label))], dtype=tf.float32)\n",
    "    return img, target_img\n",
    "\n",
    "def tf_wrapper(img, label):\n",
    "    img, target_img = tf.py_function(map_to_target, inp=[img, label], Tout=(tf.float32, tf.float32))\n",
    "    img.set_shape((None, img_height, img_width, 3))\n",
    "    target_img.set_shape((None, img_height, img_width, 3))\n",
    "    return img, target_img\n",
    "\n",
    "paired_train = data_train.map(tf_wrapper)\n",
    "paired_val = data_val.map(tf_wrapper)\n",
    "\n",
    "# --- MODEL: ENCODER-DECODER ---\n",
    "def build_image_to_image_model():\n",
    "    inputs = Input(shape=(img_height, img_width, 3))\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "    # Encoder\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    skip1 = x\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    skip2 = x\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    skip3 = x\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Cropping2D(((0, 0), (0, 0)))(x)  # Adjust shape to match skip3\n",
    "    x = layers.Concatenate()([x, skip3])\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Cropping2D(((0, 0), (0, 0)))(x)  # Adjust shape to match skip2\n",
    "    x = layers.Concatenate()([x, skip2])\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Cropping2D(((0, 0), (0, 0)))(x)  # Adjust shape to match skip1\n",
    "    x = layers.Concatenate()([x, skip1])\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    outputs = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# --- COMPILE ---\n",
    "model = build_image_to_image_model()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# --- TRAIN ---\n",
    "EPOCHS = 32\n",
    "history = model.fit(\n",
    "    paired_train,\n",
    "    validation_data=paired_val,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# --- PLOT RESULTS ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for image, label in data_val.take(1):\n",
    "    pred = model.predict(image)\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(image[i].numpy().astype(\"uint8\"))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Input\")\n",
    "        plt.subplot(2, 4, i + 5)\n",
    "        plt.imshow(pred[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Output\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- SAVE MODEL ---\n",
    "model.save(\"image_to_image_model.keras\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c932d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 13:44:25.477933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746107069.401809    9152 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746107070.490967    9152 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746107079.162532    9152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746107079.162589    9152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746107079.162595    9152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746107079.162599    9152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-01 13:44:39.908763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17125 files belonging to 25 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746107243.574640    9152 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3675 files belonging to 25 classes.\n",
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746107269.772319    9661 service.cc:152] XLA service 0x7fead00064a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746107269.772489    9661 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-05-01 13:47:50.888728: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746107272.526819    9661 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "2025-05-01 13:48:00.253026: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=2,k4=1,k5=0,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.21 = (f32[8,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,3,128,128]{3,2,1,0} %bitcast.4115, f32[32,3,3,3]{3,2,1,0} %bitcast.4106, f32[32]{0} %bitcast.4531), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1/convolution\" source_file=\"/mnt/c/Users/Pavan Sarvesh/tf-env/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-05-01 13:48:01.340568: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.08766171s\n",
      "Trying algorithm eng20{k2=2,k4=1,k5=0,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.21 = (f32[8,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,3,128,128]{3,2,1,0} %bitcast.4115, f32[32,3,3,3]{3,2,1,0} %bitcast.4106, f32[32]{0} %bitcast.4531), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1/convolution\" source_file=\"/mnt/c/Users/Pavan Sarvesh/tf-env/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-05-01 13:48:08.888758: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng5{} for conv %cudnn-conv-bw-input.12 = (f32[8,128,33,33]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,128,16,16]{3,2,1,0} %bitcast.4791, f32[128,128,3,3]{3,2,1,0} %bitcast.4780), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"functional_1/conv2d_transpose_1/conv_transpose\" source_file=\"/mnt/c/Users/Pavan Sarvesh/tf-env/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-05-01 13:48:09.216177: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.113340851s\n",
      "Trying algorithm eng5{} for conv %cudnn-conv-bw-input.12 = (f32[8,128,33,33]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,128,16,16]{3,2,1,0} %bitcast.4791, f32[128,128,3,3]{3,2,1,0} %bitcast.4780), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"functional_1/conv2d_transpose_1/conv_transpose\" source_file=\"/mnt/c/Users/Pavan Sarvesh/tf-env/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "I0000 00:00:1746107305.802178    9661 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1353/2141\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4:07\u001b[0m 314ms/step - loss: 0.3107"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input  # type:ignore\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- Enable GPU Memory Growth ---\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# CONFIG\n",
    "img_height, img_width = 128, 128\n",
    "batch_size = 8\n",
    "num_classes = 26  # 00 to 25\n",
    "\n",
    "# --- LOAD TEMPLATE IMAGES ---\n",
    "template_images = {}\n",
    "for class_idx in range(num_classes):\n",
    "    path = f\"./templates/{class_idx:02d}.png\"\n",
    "    img = tf.keras.utils.load_img(path, target_size=(img_height, img_width))\n",
    "    img = tf.keras.utils.img_to_array(img) / 255.0\n",
    "    template_images[class_idx] = img\n",
    "\n",
    "# --- DATASET LOADING ---\n",
    "def load_dataset(path):\n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        shuffle=True,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        label_mode='int'\n",
    "    )\n",
    "\n",
    "data_train = load_dataset('./dataset/newDatasetSplit/train')\n",
    "data_val = load_dataset('./dataset/newDatasetSplit/val')\n",
    "\n",
    "# --- MAP EACH INPUT TO ITS TEMPLATE IMAGE ---\n",
    "def map_to_target(img, label):\n",
    "    target_img = tf.convert_to_tensor([template_images[int(l)] for l in label.numpy()], dtype=tf.float32)\n",
    "    return img, target_img\n",
    "\n",
    "def tf_wrapper(img, label):\n",
    "    img, target_img = tf.py_function(map_to_target, inp=[img, label], Tout=(tf.float32, tf.float32))\n",
    "    img.set_shape((None, img_height, img_width, 3))\n",
    "    target_img.set_shape((None, img_height, img_width, 3))\n",
    "    return img, target_img\n",
    "\n",
    "paired_train = data_train.map(tf_wrapper).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "paired_val = data_val.map(tf_wrapper).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# --- MODEL: ENCODER-DECODER ---\n",
    "def build_image_to_image_model():\n",
    "    inputs = Input(shape=(img_height, img_width, 3))\n",
    "    x = layers.Rescaling(1.0)(inputs)  # Images already normalized\n",
    "\n",
    "    # Encoder\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    skip1 = x\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    skip2 = x\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    skip3 = x\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Concatenate()([x, skip3])\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Concatenate()([x, skip2])\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Concatenate()([x, skip1])\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    outputs = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# --- COMPILE ---\n",
    "model = build_image_to_image_model()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# --- TRAIN ---\n",
    "EPOCHS = 32\n",
    "history = model.fit(\n",
    "    paired_train,\n",
    "    validation_data=paired_val,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# --- PLOT RESULTS ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for image, label in data_val.take(1):\n",
    "    pred = model.predict(image / 255.0)\n",
    "    for i in range(min(4, len(image))):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(image[i].numpy().astype(\"uint8\"))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Input\")\n",
    "        plt.subplot(2, 4, i + 5)\n",
    "        plt.imshow(pred[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Output\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- SAVE MODEL ---\n",
    "model.save(\"image_to_image_model.keras\")\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f2075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
